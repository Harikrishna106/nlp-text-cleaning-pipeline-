{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b30faf59-902a-4d2b-a634-a6d48e1e44a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Salapakshi\n",
      "[nltk_data]     suneel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Salapakshi\n",
      "[nltk_data]     suneel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Salapakshi\n",
      "[nltk_data]     suneel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download('punkt_tab')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f7d4d6-3836-43c9-bd29-b0443eae3300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c19a7fd-0afa-4dc5-925c-5d91170ab97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e2f4cb9-28d9-4ba2-a6c1-a34f46cb220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Srsly.!!!, this wuz. like, the! WORST! ideA ever??!! I dun even kno what Im sUpposed 2 b doin or why Im reAdin this!!!\n",
    "its soooooooooooooooooooooooooo Messy u cant evEn tell where the sentEnces begin/end n the pUnctuAtiOn? its just a nIghtmare\n",
    "of @#^ & *%^ characters. like. WHO writes like this?!! its makin my head hurt!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07a1efba-9814-4cff-900c-e9e437df1c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'srsly this wuz like the worst idea ever i dun even kno what im supposed  b doin or why im readin this\\nits soooooooooooooooooooooooooo messy u cant even tell where the sentences beginend n the punctuation its just a nightmare\\nof    characters like who writes like this its makin my head hurt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([ch for ch in text if ch not in string.punctuation and not ch.isdigit()])\n",
    "    return text\n",
    "cleaned_text = preprocess(text)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25a9b84a-dc52-4fc5-a6ac-bbde9cf0ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(cleaned_text)\n",
    "tokens = [word for word in tokens if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5546f040-6e04-4313-af23-5e631f30b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(' '.join(tokens))\n",
    "lemmatized = [token.lemma_ for token in doc if not token.is_stop]\n",
    "final_text = ' '.join(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d95b62b4-9f61-4529-bcf2-22adf304459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dun  -  PERSON\n",
      "kno  -  PERSON\n",
      "2  -  CARDINAL\n",
      "Messy  -  PERSON\n",
      "pUnctuAtiOn  -  ORG\n",
      "nIghtmare  -  ORG\n",
      "@#^ & *%  -  ORG\n"
     ]
    }
   ],
   "source": [
    "doc_ner = nlp(text)\n",
    "for ent in doc_ner.ents:\n",
    "    print(ent.text, \" - \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "312fab50-8ce1-4e8e-86cb-6dfb5664e005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad' 'beginend' 'character' 'doin' 'dun' 'head' 'hurt' 'idea' 'kno'\n",
      " 'like' 'makin' 'messy' 'nightmare' 'not' 'punctuation' 'readin'\n",
      " 'sentence' 'soooooooooooooooooooooooooo' 'srsly' 'suppose' 'tell' 'write'\n",
      " 'wuz']\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform([final_text])\n",
    "print(tfidf.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "901185ca-a582-41b1-86cd-b62e2e0f85e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1796053  0.1796053  0.1796053  0.1796053  0.1796053  0.1796053\n",
      "  0.1796053  0.1796053  0.1796053  0.53881591 0.1796053  0.1796053\n",
      "  0.1796053  0.1796053  0.1796053  0.1796053  0.1796053  0.1796053\n",
      "  0.1796053  0.1796053  0.1796053  0.1796053  0.1796053 ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9528a1b-8061-456b-8969-cbe88a327715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Sentiment Polarity\n",
      "-0.6953125\n",
      "Step 6: Sentiment Subjectivity\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(text)\n",
    "print(\"Step 6: Sentiment Polarity\")\n",
    "print(blob.sentiment.polarity)\n",
    "\n",
    "print(\"Step 6: Sentiment Subjectivity\")\n",
    "print(blob.sentiment.subjectivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6fbf068-f0dd-4c54-a704-69f1e747590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formal Sentence:\n",
      "Bad begin or end character doing don’t head hurt idea know like making mess nightmare not punctuation reading sentence extremely seriously supposed tell write was.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "tokens = ['bad', 'beginend', 'character', 'doin', 'dun', 'head', 'hurt', 'idea', 'kno',\n",
    "          'like', 'makin', 'messy', 'nightmare', 'not', 'punctuation', 'readin',\n",
    "          'sentence', 'soooooooooooooooooooooooooo', 'srsly', 'suppose', 'tell', 'write',\n",
    "          'wuz']\n",
    "\n",
    "rough_text = ' '.join(tokens)\n",
    "\n",
    "slang_dict = {\n",
    "    \"srsly\": \"seriously\",\n",
    "    \"wuz\": \"was\",\n",
    "    \"dun\": \"don’t\",\n",
    "    \"kno\": \"know\",\n",
    "    \"doin\": \"doing\",\n",
    "    \"makin\": \"making\",\n",
    "    \"readin\": \"reading\",\n",
    "    \"soooooooooooooooooooooooooo\": \"extremely\",\n",
    "    \"suppose\": \"supposed\",\n",
    "    \"beginend\": \"begin or end\"\n",
    "}\n",
    "\n",
    "for slang, formal in slang_dict.items():\n",
    "    rough_text = rough_text.replace(slang, formal)\n",
    "\n",
    "blob = TextBlob(rough_text)\n",
    "corrected = str(blob.correct())\n",
    "final_output = corrected.capitalize() + '.'\n",
    "\n",
    "print(\"Formal Sentence:\")\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c97b2e-6aeb-4846-92fd-888953ba7d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
